{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "import math\n",
    "from threading import Thread\n",
    "from queue import Queue\n",
    "import time\n",
    "import string\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import os\n",
    "from bs4 import BeautifulSoup, Tag, NavigableString\n",
    "import tempfile\n",
    "import configparser\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "import sys\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_config =\"\"\"\n",
    "[requests]\n",
    "service_URL_country_codes = be,ca,dk,fi,fr,de,gr,hk,it,jp,no,sk,si,se,ch,es,com,co.uk\n",
    "words_per_request = 1\n",
    "\n",
    "[translation]\n",
    "ignorable_punctuation_tokens = !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~«»‘’”“–\n",
    "\n",
    "[formatting]\n",
    "class_translation = il_translation\n",
    "class_word = il_word\n",
    "class_paragraph = il_paragraph\n",
    "class_space = il_space\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_interlinear_css =\"\"\"\n",
    ".il_paragraph {\n",
    "    width: 100%;\n",
    "    overflow-wrap: anywhere;\n",
    "    font-size: 1px;\n",
    "}\n",
    "\n",
    ".il_word {\n",
    "    font-size: 16px;\n",
    "    position: relative;\n",
    "    display: inline-block;\n",
    "    padding-left: 12px;\n",
    "    padding-right: 12px;\n",
    "    text-align: center;\n",
    "    overflow-wrap: normal;\n",
    "    height: 70px;\n",
    "}\n",
    "\n",
    ".il_word:first-child {\n",
    "    padding-left: 0px;\n",
    "}\n",
    "\n",
    ".il_word:last-child {\n",
    "    padding-right: 0px;\n",
    "}\n",
    "\n",
    ".il_translation {\n",
    "    width: 100%;\n",
    "    position: absolute;\n",
    "    top: 30px;\n",
    "    left: 0;\n",
    "    font-size: 12px;\n",
    "    text-align: center;\n",
    "    color: #999;\n",
    "\n",
    "    -webkit-user-select: none;  \n",
    "    -moz-user-select: none;    \n",
    "    -ms-user-select: none;      \n",
    "    user-select: none;\n",
    "    /* line-height: 0; */\n",
    "    overflow-wrap: normal;\n",
    "}\n",
    "\n",
    ".not-word, .missing-translation {\n",
    "    padding-left: 0;\n",
    "    padding-right: 0;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    # First check in the current directory \n",
    "    config_path = Path(\".\", 'interlinearize.config')\n",
    "    if not config_path.is_file():\n",
    "        # Otherwise check in the app settings directory\n",
    "        config_path = Path(settings_path, 'interlinearize.config')\n",
    "        \n",
    "        # And if that doesn't work, create a new config file in the app settings directory\n",
    "        if not config_path.is_file():\n",
    "            Path(settings_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            with open(str(config_path), \"w\") as f:\n",
    "                f.write(default_config)\n",
    "    \n",
    "    config = configparser.RawConfigParser()\n",
    "    config.read(str(config_path))\n",
    "    return config\n",
    "\n",
    "def load_word_dict(src, dest):\n",
    "    # First check in the current directory\n",
    "    word_dict_folder_path = Path(\".\", 'dicts')\n",
    "    \n",
    "    if not word_dict_folder_path.is_dir():\n",
    "        # Otherwise check in the app settings directory\n",
    "        word_dict_folder_path = Path(settings_path, 'dicts')\n",
    "        \n",
    "    if word_dict_folder_path.is_dir():\n",
    "        \n",
    "        # Check if src_dest.txt is there\n",
    "        word_dict_path = Path(word_dict_folder_path, \"%s_%s.txt\" % (src,dest))\n",
    "        \n",
    "        if word_dict_path.is_file():\n",
    "            with open(str(word_dict_path), \"r\") as csv_file:\n",
    "                csv_r = csv.reader(csv_file, delimiter='\\t')\n",
    "                word_dict = {}\n",
    "\n",
    "                for src_w, dest_w in csv_r:\n",
    "                    word_dict[src_w] = dest_w\n",
    "                    \n",
    "                return str(word_dict_folder_path), word_dict\n",
    "        else:\n",
    "            return str(word_dict_folder_path), {}\n",
    "    else:\n",
    "        return str(word_dict_folder_path), {}\n",
    "    \n",
    "def save_word_dict(src, dest, word_dict):\n",
    "    word_dict_folder_path, _ = load_word_dict(src, dest)\n",
    "    Path(word_dict_folder_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    word_dict_path = Path(word_dict_folder_path, \"%s_%s.txt\" % (src, dest))\n",
    "\n",
    "    src_words = list(word_dict.keys())\n",
    "    dest_words = [word_dict[w] for w in src_words]\n",
    "\n",
    "    with open(str(word_dict_path), \"w\") as csv_file:\n",
    "        csv_w = csv.writer(csv_file, delimiter='\\t', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "        for src_w, dest_w in zip(src_words, dest_words):\n",
    "            csv_w.writerow([src_w, dest_w])\n",
    "    \n",
    "def get_interlinear_css():\n",
    "    # First check in the current directory \n",
    "    css_path = Path(\".\", 'interlinear.css')\n",
    "    if not css_path.is_file():\n",
    "        # Otherwise check in the app settings directory\n",
    "        css_path = Path(settings_path, 'interlinear.css')\n",
    "        \n",
    "        # And if that doesn't work, create a new config file in the app settings directory\n",
    "        if not css_path.is_file():\n",
    "            Path(settings_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            with open(str(css_path), \"w\") as f:\n",
    "                f.write(default_interlinear_css)\n",
    "    \n",
    "    with open(str(css_path), \"r\") as f:\n",
    "        css = f.read()\n",
    "    \n",
    "    return css\n",
    "\n",
    "def convert_book_to_HTML(book_path):\n",
    "    \"\"\"Takes any file supported by ebook-convert, and converts it into an HTML file and returns it.\"\"\"\n",
    "    tmp_dir = tempfile.TemporaryDirectory()\n",
    "    tmp_dir_path = tmp_dir.name\n",
    "\n",
    "    book_title = Path(book_path).stem\n",
    "    htmlz_title = book_title + \".htmlz\"\n",
    "\n",
    "    # Convert\n",
    "    process = subprocess.Popen(['ebook-convert', book_path, os.path.join(tmp_dir_path, htmlz_title) ],\n",
    "                         stdout=subprocess.PIPE, \n",
    "                         stderr=subprocess.PIPE)\n",
    "    stdout, stderr = process.communicate()\n",
    "\n",
    "    if stderr:\n",
    "        er_msg = \"Error from ebook-convert when converting the input. Error message:\\n\\n\\t\" + stderr.decode(\"utf-8\")\n",
    "        raise Exception(er_msg)\n",
    "\n",
    "    # Create folder for unzip\n",
    "    process = subprocess.Popen(['mkdir', os.path.join(tmp_dir_path, book_title)],\n",
    "                         stdout=subprocess.PIPE, \n",
    "                         stderr=subprocess.PIPE)\n",
    "    stdout, stderr = process.communicate()\n",
    "\n",
    "    if stderr:\n",
    "        er_msg = \"Error:\\n\\n\\t\" + stderr.decode(\"utf-8\")\n",
    "        raise Exception(er_msg)\n",
    "\n",
    "    # Unzip\n",
    "    process = subprocess.Popen(['unzip', os.path.join(tmp_dir_path, htmlz_title), \"-d\", os.path.join(tmp_dir_path, book_title)],\n",
    "                         stdout=subprocess.PIPE, \n",
    "                         stderr=subprocess.PIPE)\n",
    "    stdout, stderr = process.communicate()\n",
    "\n",
    "    if stderr:\n",
    "        er_msg = \"Error:\\n\\n\\t\" + stderr.decode(\"utf-8\")\n",
    "        raise Exception(er_msg)\n",
    "\n",
    "    book_html_path = os.path.join(tmp_dir_path, book_title, \"index.html\")\n",
    "    page = open(book_html_path)\n",
    "    soup = BeautifulSoup(page.read(), \"html.parser\")\n",
    "    #tmp_dir.cleanup()\n",
    "    return tmp_dir, soup\n",
    "    \n",
    "def get_word_list(text, ignorable_punctuation_tokens):\n",
    "    \"\"\"Gets a list of unique words in the given text, using\n",
    "    the NLTK tokenizer.\"\"\"\n",
    "    words = word_tokenize(text, language='french')\n",
    "    words = list(set([w.lower() for w in words]))\n",
    "    new_words = []\n",
    "    \n",
    "    for w in words:\n",
    "        if w.isnumeric():\n",
    "            continue\n",
    "        if len(set(w) - ignorable_punctuation_tokens) == 0: # Check if word is just punctuation\n",
    "            continue\n",
    "            \n",
    "        new_words.append(w)\n",
    "    return new_words\n",
    "\n",
    "def lookup_word(word_dict, word, ignorable_punctuation_tokens):\n",
    "    \"\"\"Given a word dictionary, return the word's translation.\n",
    "    Using `ignorable_punctuation_tokens`, any prefixed or affixed\n",
    "    punctuation is removed.\n",
    "    \n",
    "    Error codes:\n",
    "    0 - Not a word (just numbers, or punctuation)\n",
    "    1 - Missing translation\"\"\"\n",
    "    w_tok = word_tokenize(word, language='french')\n",
    "    \n",
    "    w = None\n",
    "    \n",
    "    for _w in w_tok:\n",
    "        if _w.isnumeric():\n",
    "            continue\n",
    "        if len(set(_w) - ignorable_punctuation_tokens) == 0:\n",
    "            continue\n",
    "        w = _w.lower()\n",
    "        \n",
    "    if not w is None:\n",
    "        if w in word_dict:\n",
    "            return word_dict[w]\n",
    "        else:\n",
    "            return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def construct_word_list_from_text(words, word_dict, src, dest, service_urls, words_per_request):\n",
    "    \"\"\"Adds entries to the word_dict by translating the words in the given text.\n",
    "    The function uses googletrans.Translator to get the translations of each words.\n",
    "    `service_urls` is a list of URLs that the requests are divided over.\"\"\"\n",
    "    \n",
    "    # Remove words that are already in word_dict\n",
    "    \n",
    "    words_already_found = set(word_dict.keys())\n",
    "    words = list( set(words) - words_already_found )\n",
    "    \n",
    "    # Translate!\n",
    "\n",
    "    words_per_thread = int(len(words) / len(service_urls))\n",
    "\n",
    "    thread_word_list = []\n",
    "    for i in range(len(service_urls)):\n",
    "        if i != len(service_urls) - 1:\n",
    "            thread_word_list.append( words[ i*words_per_thread: (i+1)*words_per_thread ] )\n",
    "        else:\n",
    "            thread_word_list.append( words[ i*words_per_thread : ] )\n",
    "\n",
    "    def translate_words(words, src, dest, words_per_request, service_url, que):\n",
    "        t1 = time.time()\n",
    "        translations = {}\n",
    "\n",
    "        translator = Translator(service_urls=[service_url])\n",
    "\n",
    "        for i in range( math.ceil(len(words) / words_per_request) ):\n",
    "            if (i+1)*words_per_request <= len(words):\n",
    "                words_to_translate = words[ i*words_per_request : (i+1)*words_per_request ]\n",
    "            else:\n",
    "                words_to_translate = words[ i*words_per_request :  ]\n",
    "\n",
    "            ts = translator.translate(words_to_translate, src=src, dest=dest)\n",
    "\n",
    "            for translation in ts:\n",
    "                translations[translation.origin] = translation.text.lower()\n",
    "\n",
    "        que.put(translations)\n",
    "        t2 = time.time()\n",
    "\n",
    "\n",
    "    que = Queue()\n",
    "\n",
    "    que_times = Queue()\n",
    "\n",
    "    threads = []\n",
    "    for thread_words, service_url in zip(thread_word_list, service_urls):\n",
    "        thread = Thread(target=translate_words, args=( thread_words, src, dest, words_per_request, service_url, que ))\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "        \n",
    "    for i in range(len(threads)):\n",
    "        t_dict = que.get()\n",
    "        word_dict.update(t_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_subtitle_to_text(text, word_dict, class_translation, class_word, class_paragraph, class_space):\n",
    "    word_list = str(text).split()\n",
    "    tag_list = []\n",
    "\n",
    "    for w in word_list:\n",
    "        w_translated = lookup_word(word_dict, w, ignorable_punctuation_tokens)\n",
    "        trans_status = \"\"\n",
    "        word_status = \"\"\n",
    "\n",
    "        if w_translated == 0:\n",
    "            trans_status = \"not-word\"\n",
    "            word_status = \"not-word\"\n",
    "            w_translated = \"\"\n",
    "        elif w_translated == 1:\n",
    "            trans_status = \"missing-translation\"\n",
    "            word_status = \"missing-translation\"\n",
    "            w_translated = \"\"\n",
    "        \n",
    "        word_span = Tag(builder=book_soup.builder,  name='div', attrs={'class':class_word + \" \" + word_status})\n",
    "        trans_div = Tag(builder=book_soup.builder,  name='div', attrs={'class':class_translation + \" \" + trans_status})\n",
    "        trans_div.insert(0, w_translated)\n",
    "        word_span.insert(0, trans_div)\n",
    "        word_span.insert(0, w)    \n",
    "        tag_list.append(word_span)\n",
    "    \n",
    "        space_tag = Tag(builder=book_soup.builder,  name='div', attrs={'class':class_space})\n",
    "        space_tag.insert(0, \"&nbsp;\")\n",
    "        tag_list.append(NavigableString(\" \"))\n",
    "        \n",
    "    if len(word_list) > 0:\n",
    "        word_list = word_list[:-1] # Remove last space\n",
    "    \n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_subtitle_to_soup(book_soup, word_dict, class_translation, class_word, class_paragraph, class_space):\n",
    "    for paragraph in book_soup.find_all(\"p\"):\n",
    "\n",
    "        new_paragraph_contents = []\n",
    "\n",
    "        for i in range(len(paragraph)):\n",
    "            elem = paragraph.contents[i]\n",
    "\n",
    "            # Convert text directly in <p>\n",
    "            if type(elem) == NavigableString:\n",
    "                tag_list = add_subtitle_to_text(elem, word_dict, class_translation, class_word, class_paragraph, class_space)\n",
    "                new_paragraph_contents.append(tag_list)\n",
    "            elif type(elem) == Tag and elem.name == 'span':\n",
    "                tag_list = add_subtitle_to_text(elem.text, word_dict, class_translation, class_word, class_paragraph, class_space)\n",
    "                # Replace contents inside span with the subtitled content\n",
    "                elem.clear()\n",
    "                for t in tag_list:\n",
    "                    elem.insert( len(elem.contents), t )\n",
    "                #new_paragraph_contents.append([elem])\n",
    "                new_paragraph_contents.append(tag_list)\n",
    "            else:\n",
    "                new_paragraph_contents.append([elem])\n",
    "                continue\n",
    "\n",
    "        paragraph.name = \"div\"\n",
    "        paragraph['class'] = paragraph.get('class', '') + [class_paragraph]\n",
    "        paragraph.clear()\n",
    "\n",
    "        for elem_l in new_paragraph_contents:\n",
    "            for elem in elem_l:\n",
    "                paragraph.insert( len(paragraph.contents), elem )\n",
    "\n",
    "    book_soup.find('head').insert(0, Tag(builder=book_soup.builder,  name='link', attrs={'rel':'stylesheet', 'href' : 'interlinear.css'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_translation(book_soup, tmp, book_path, out_path):\n",
    "    ## Overwrite the original index.html of the book\n",
    "\n",
    "    tmp_dir_path = tmp_dir.name\n",
    "    book_title = Path(book_path).stem\n",
    "\n",
    "    book_path = os.path.join(tmp_dir_path, book_title)\n",
    "    book_html_path = os.path.join(tmp_dir_path, book_title, \"index.html\")\n",
    "\n",
    "    with open(book_html_path, \"w\") as f:\n",
    "        f.write(str(book_soup))\n",
    "\n",
    "    ## Copy the interlinear.css file to the folder\n",
    "\n",
    "    with open(os.path.join(book_path, \"interlinear.css\"), \"w\") as f:\n",
    "        f.write(get_interlinear_css())\n",
    "\n",
    "    ## Convert index.html to the desired output\n",
    "    \n",
    "    # If the output format is not specified, then the output will just be the copied folder\n",
    "    \n",
    "    if Path(out_path).suffix == \"\":\n",
    "        process = subprocess.Popen(['cp', '-r', book_path, out_path ],\n",
    "                             stdout=subprocess.PIPE, \n",
    "                             stderr=subprocess.PIPE)\n",
    "        stdout, stderr = process.communicate()\n",
    "\n",
    "    \n",
    "    # else, use calibre\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        process = subprocess.Popen(['ebook-convert', book_html_path, out_path ],\n",
    "                             stdout=subprocess.PIPE, \n",
    "                             stderr=subprocess.PIPE)\n",
    "        stdout, stderr = process.communicate()\n",
    "\n",
    "        if stderr:\n",
    "            er_msg = \"Error from ebook-convert. Error message:\\n\\n\\t\" + stderr.decode(\"utf-8\")\n",
    "            raise Exception(er_msg)\n",
    "\n",
    "    tmp_dir.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_interactive():\n",
    "    import __main__ as main\n",
    "    return not hasattr(main, '__file__')\n",
    "\n",
    "if is_interactive():\n",
    "    src_lan = \"fr\"\n",
    "    dest_lan = \"en\"\n",
    "    book_path = \"examples/original/Candide - Voltaire.epub\"\n",
    "    out_path = \"examples/interlinearized/Candide - Voltaire (interlinearized).epub\"\n",
    "else:\n",
    "    src_lan, dest_lan, book_path, out_path = sys.argv[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_path = str(Path(Path.home(), '.interlinearize'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if book exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(book_path).is_file():\n",
    "    print(\"'%s' cannot be found.\" % book_path)\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()\n",
    "\n",
    "## requests\n",
    "\n",
    "country_codes = config['requests']['service_URL_country_codes'].split(\",\")\n",
    "service_urls = []\n",
    "for co in country_codes:\n",
    "    service_urls.append( \"translate.google.%s\" % co )\n",
    "    \n",
    "words_per_request = int( config['requests']['words_per_request'] )\n",
    "\n",
    "## translation\n",
    "\n",
    "ignorable_punctuation_tokens = set(config['translation']['ignorable_punctuation_tokens'])\n",
    "\n",
    "## formatting\n",
    "\n",
    "class_translation = config['formatting']['class_translation']\n",
    "class_word = config['formatting']['class_word']\n",
    "class_paragraph = config['formatting']['class_paragraph']\n",
    "class_space = config['formatting']['class_space']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Converting book to HTMLZ\")\n",
    "tmp_dir, book_soup = convert_book_to_HTML(book_path)\n",
    "\n",
    "_, word_dict = load_word_dict(src_lan, dest_lan)\n",
    "\n",
    "word_list = get_word_list(book_soup.text, ignorable_punctuation_tokens)\n",
    "\n",
    "print(\"Finding translations of new words\")\n",
    "construct_word_list_from_text(word_list, word_dict, src_lan, dest_lan, service_urls, words_per_request)\n",
    "\n",
    "save_word_dict(src_lan, dest_lan, word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct translated book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Constructing interlinearized version\")\n",
    "\n",
    "add_subtitle_to_soup(book_soup, word_dict, class_translation, class_word, class_paragraph, class_space)\n",
    "\n",
    "write_translation(book_soup, tmp_dir, book_path, out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}